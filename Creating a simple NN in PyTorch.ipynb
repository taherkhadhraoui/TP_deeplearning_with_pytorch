{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many libraries out there that we can use for our deep learning cravings. And I’ve tried a few of them. When I started, I tried tensorflow. And then I tried Keras. And then i tried PyTorch. And if I’m being completely honest to myself I like PyTorch the most.\n",
    "\n",
    "I think the reason I like PyTorch much more than Tensorflow, is that PyTorch is very pythonic. It uses the style and powers of python, which makes it much easier and simpler to understand and use. Especially for me, because I code mainly in Python. The problem with Tensorflow is that it requires you to learn a lot of *Tensorflow-specific jargon*. This example is about how you can create a simple neural network in PyTorch.\n",
    "\n",
    "We’ll create a simple neural network with one hidden layer and a single output unit. We will use the ReLU activation in the hidden layer and the sigmoid activation in the output layer.\n",
    "First, we need to import the PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the sizes of all the layers and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining input size, hidden layer size, output size and batch size respectively\n",
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we create some dummy input data x and some dummy target data y . We use PyTorch Tensors to store this data. PyTorch Tensors can be used and manipulated just like NumPy arrays but with the added benefit that PyTorch tensors can be run on the GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input and target tensors (data)\n",
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, for the main course, we will define our model in one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a model that looks like input -> linear -> relu -> linear -> sigmoid. There is another way to define our models which is used to define more complicated and custom models. It is done by defining our model in a class. You can read about it [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules).\n",
    "\n",
    "Now, it is time to construct our loss function. We will use the Mean Squared Error Loss.\n",
    "\n",
    "Also, don’t forget to define our optimizer. We use Stochastic Gradient Descent in this one and a learning rate of 0.01. model.parameters() returns an iterator over our model’s parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, for the dessert, we run our Gradient Descent for 50 epochs. This does the forward propagation, loss computation, backward propagation and parameter updation in that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.23753903806209564\n",
      "epoch:  1  loss:  0.23736728727817535\n",
      "epoch:  2  loss:  0.2371959239244461\n",
      "epoch:  3  loss:  0.23702484369277954\n",
      "epoch:  4  loss:  0.2368541806936264\n",
      "epoch:  5  loss:  0.23668383061885834\n",
      "epoch:  6  loss:  0.23651380836963654\n",
      "epoch:  7  loss:  0.2363440990447998\n",
      "epoch:  8  loss:  0.23617476224899292\n",
      "epoch:  9  loss:  0.2360057532787323\n",
      "epoch:  10  loss:  0.23583707213401794\n",
      "epoch:  11  loss:  0.23566871881484985\n",
      "epoch:  12  loss:  0.23550067842006683\n",
      "epoch:  13  loss:  0.23533299565315247\n",
      "epoch:  14  loss:  0.23516564071178436\n",
      "epoch:  15  loss:  0.23499858379364014\n",
      "epoch:  16  loss:  0.23483186960220337\n",
      "epoch:  17  loss:  0.23466545343399048\n",
      "epoch:  18  loss:  0.23449937999248505\n",
      "epoch:  19  loss:  0.2343336045742035\n",
      "epoch:  20  loss:  0.2341681569814682\n",
      "epoch:  21  loss:  0.2340030074119568\n",
      "epoch:  22  loss:  0.23383818566799164\n",
      "epoch:  23  loss:  0.23367366194725037\n",
      "epoch:  24  loss:  0.23350946605205536\n",
      "epoch:  25  loss:  0.23334555327892303\n",
      "epoch:  26  loss:  0.23318195343017578\n",
      "epoch:  27  loss:  0.2330186665058136\n",
      "epoch:  28  loss:  0.2328556627035141\n",
      "epoch:  29  loss:  0.23269298672676086\n",
      "epoch:  30  loss:  0.23253057897090912\n",
      "epoch:  31  loss:  0.23236848413944244\n",
      "epoch:  32  loss:  0.23220668733119965\n",
      "epoch:  33  loss:  0.23204517364501953\n",
      "epoch:  34  loss:  0.2318839728832245\n",
      "epoch:  35  loss:  0.23172304034233093\n",
      "epoch:  36  loss:  0.23156242072582245\n",
      "epoch:  37  loss:  0.23140208423137665\n",
      "epoch:  38  loss:  0.23124200105667114\n",
      "epoch:  39  loss:  0.2310822457075119\n",
      "epoch:  40  loss:  0.23092272877693176\n",
      "epoch:  41  loss:  0.2307635396718979\n",
      "epoch:  42  loss:  0.23060458898544312\n",
      "epoch:  43  loss:  0.23044593632221222\n",
      "epoch:  44  loss:  0.230287566781044\n",
      "epoch:  45  loss:  0.23012948036193848\n",
      "epoch:  46  loss:  0.22997163236141205\n",
      "epoch:  47  loss:  0.2298140823841095\n",
      "epoch:  48  loss:  0.22965680062770844\n",
      "epoch:  49  loss:  0.22949977219104767\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "for epoch in range(50):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*y_pred* gets the predicted values from a forward pass of our model. We pass this, along with target values y to the criterion which calculates the loss. Then, *optimizer.zero_grad()* zeroes out all the gradients. We need to do this so that previous gradients don’t keep on accumulating. Then, *loss.backward()* is the main PyTorch magic that uses PyTorch’s Autograd feature. Autograd computes all the gradients w.r.t. all the parameters automatically based on the computation graph that it creates dynamically. Basically, this does the backward pass (backpropagation)of gradient descent. Finally, we call *optimizer.step()* which does a single updation of all the parameters using the new gradients.\n",
    "\n",
    "And that’s it. We have successfully trained a simple two-layer neural network in PyTorch and we didn’t really have to go through a ton of random jargon to do it. PyTorch keeps it sweet and simple, just the way everyone likes it.\n",
    "\n",
    "If you want to learn more about PyTorch and want to dive deeper into it, take a look at PyTorch’s official documentation and tutorials. They are really well-written. You can find them [here](https://pytorch.org/tutorials/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
